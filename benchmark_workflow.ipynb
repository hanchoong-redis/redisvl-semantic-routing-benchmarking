{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a6ba1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q sentence-transformers ranx redisvl \"redis-retrieval-optimizer>=0.2.0\" datasets pandas openai scikit-learn h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ecff2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7b92b",
   "metadata": {},
   "source": [
    "## Run a Redis instance\n",
    "\n",
    "#### Mac\n",
    "\n",
    "Remember to point at 6380, not 6379\n",
    "```bash\n",
    "docker run -d --name redis-stack \\\n",
    "  -p 6380:6379 -p 8001:8001 \\\n",
    "  -v redis-data:/data \\\n",
    "  redis/redis-stack:latest\n",
    "\n",
    "```\n",
    "#### For Colab\n",
    "Use the shell script below to download, extract, and install [Redis Stack](https://redis.io/docs/getting-started/install-stack/) directly from the Redis package archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "%%sh\n",
    "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
    "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
    "sudo apt-get update  > /dev/null 2>&1\n",
    "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
    "redis-stack-server --daemonize yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce384584",
   "metadata": {},
   "source": [
    "### Define the Redis Connection URL\n",
    "\n",
    "By default this notebook connects to the local instance of Redis Stack. **If you have your own Redis Enterprise instance** - replace REDIS_PASSWORD, REDIS_HOST and REDIS_PORT values with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414b8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, io\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Replace values below with your own if using Redis Cloud instance\n",
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\") # ex: \"redis-18374.c253.us-central1-1.gce.cloud.redislabs.com\"\n",
    "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6380\")      # ex: 18374\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")  \n",
    "REDIS_DB = os.getenv(\"REDIS_DB\", 0)\n",
    "\n",
    "# If SSL is enabled on the endpoint, use rediss:// as the URL prefix\n",
    "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe5691",
   "metadata": {},
   "source": [
    "# Set Credentials for OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3fa9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c083da",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20433a8f",
   "metadata": {},
   "source": [
    "## Metrics Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c41b05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def format_timestamp(ts=None):\n",
    "    if ts is None:\n",
    "        ts = datetime.now()\n",
    "    elif isinstance(ts, (int, float)):\n",
    "        ts = datetime.fromtimestamp(ts)\n",
    "    return ts.strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.timings = {}\n",
    "\n",
    "    def timeit(self, name):\n",
    "        class _T:\n",
    "            def __enter__(_self):\n",
    "                _self.start = time.perf_counter()\n",
    "            def __exit__(_self, *a):\n",
    "                end=round(time.perf_counter() - _self.start, 3)\n",
    "                self.timings[name] = end\n",
    "        return _T()\n",
    "\n",
    "def classification_metrics(preds: list[str], labels: list[str]) -> dict:\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision_macro\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"confusion_matrix\": confusion_matrix(labels, preds).tolist(),\n",
    "        \"classification_report\": classification_report(labels, preds, zero_division=0, output_dict=True)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_experiment_summary(eval_metrics, \n",
    "                             router_config, timings, redis_schema=None, redis_index_info=None,\n",
    "                             experiment_notes=\"NA\", experiment_name=\"experiment\", \n",
    "                             print_output=True, save_output=True):\n",
    "    lines = []\n",
    "    lines.append(\"=== Experiment Summary: Semantic Routing Classification ===\\n\")\n",
    "\n",
    "    lines.append(f\"Timestamp         : {format_timestamp()}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    lines.append(f\"Experiment Notes  : {experiment_notes}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # Data Config\n",
    "    lines.append(\"DATA CONFIGURATION\")\n",
    "    lines.append(f\"Router Name       : {router_config.get('name')}\")\n",
    "    lines.append(f\"Batch Size        : {router_config.get('batch_size')}\")\n",
    "    lines.append(f\"Dataset Repo ID    : {router_config.get('dataset_repo_id')}\")\n",
    "    lines.append(f\"Dataset Domain    : {router_config.get('dataset_domain')}\")\n",
    "    lines.append(f\"Dataset Name      : {router_config.get('dataset_name')}\")\n",
    "    lines.append(f\"Train Limit       : {router_config.get('train_limit')}\")\n",
    "    lines.append(f\"Test Limit        : {router_config.get('test_limit')}\")\n",
    "    vec_cfg = router_config.get('vectorizer_config', {})\n",
    "    lines.append(f\"Vector Dimensions : {vec_cfg.get('dimensions')}\")\n",
    "    lines.append(f\"Embedding Model   : {vec_cfg.get('OPENAI_MODEL')}\\n\")\n",
    "\n",
    "    # Redis Schema\n",
    "    if redis_schema:\n",
    "        idx = redis_schema.get('index', {})\n",
    "        lines.append(\"REDIS SCHEMA\")\n",
    "        lines.append(f\"Index Name        : {idx.get('name')}\")\n",
    "        lines.append(f\"Prefix            : {idx.get('prefix')}\")\n",
    "        lines.append(f\"Storage Type      : {idx.get('storage_type')}\")\n",
    "        lines.append(\"Fields:\")\n",
    "        for f in redis_schema.get('fields', []):\n",
    "            if f['type'] == 'vector':\n",
    "                attrs = f.get('attrs', {})\n",
    "                lines.append(f\"  - {f['name']} ({f['type']})  dims={attrs.get('dims')}, metric={attrs.get('distance_metric')}, algo={attrs.get('algorithm')}\")\n",
    "            else:\n",
    "                lines.append(f\"  - {f['name']} ({f['type']})\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # Redis Index Info\n",
    "    if redis_index_info:\n",
    "        lines.append(\"REDIS INDEX INFO\")\n",
    "        lines.append(f\"Documents         : {redis_index_info['num_docs']}\")\n",
    "        lines.append(f\"Max Doc ID        : {redis_index_info['max_doc_id']}\")\n",
    "        lines.append(f\"Num Terms         : {redis_index_info['num_terms']}\")\n",
    "        lines.append(f\"Num Records       : {redis_index_info['num_records']}\")\n",
    "        lines.append(f\"Inverted Size (MB): {float(redis_index_info['inverted_sz_mb']):.6f}\")\n",
    "        lines.append(f\"Vector Index (MB) : {float(redis_index_info['vector_index_sz_mb']):.6f}\\n\")\n",
    "    else:\n",
    "        lines.append(\"No index info available\")\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    lines.append(\"EVALUATION METRICS\")\n",
    "    lines.append(f\"Accuracy          : {eval_metrics['accuracy']:.4f}\")\n",
    "    lines.append(f\"Precision (macro) : {eval_metrics['precision_macro']:.4f}\")\n",
    "    lines.append(f\"Recall (macro)    : {eval_metrics['recall_macro']:.4f}\")\n",
    "    lines.append(f\"F1 (macro)        : {eval_metrics['f1_macro']:.4f}\\n\")\n",
    "\n",
    "    cm = eval_metrics['confusion_matrix']\n",
    "    lines.append(\"CONFUSION MATRIX [ [TP_0, FP_0], [FN_1, TP_1] ]\")\n",
    "    lines.append(f\"  {cm[0]}\")\n",
    "    lines.append(f\"  {cm[1]}\\n\")\n",
    "\n",
    "    lines.append(\"CLASSIFICATION REPORT\")\n",
    "    report = eval_metrics['classification_report']\n",
    "    for label, metrics in report.items():\n",
    "        if label in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            continue\n",
    "        lines.append(f\"Class {label}: \"\n",
    "                     f\"Precision={metrics['precision']:.3f}, \"\n",
    "                     f\"Recall={metrics['recall']:.3f}, \"\n",
    "                     f\"F1={metrics['f1-score']:.3f}, \"\n",
    "                     f\"Support={int(metrics['support'])}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    macro = report.get('macro avg', {})\n",
    "    lines.append(\"Macro Avg:\")\n",
    "    lines.append(f\"  Precision={macro.get('precision', 0):.3f}, \"\n",
    "                 f\"Recall={macro.get('recall', 0):.3f}, \"\n",
    "                 f\"F1={macro.get('f1-score', 0):.3f}\\n\")\n",
    "\n",
    "    # Timings\n",
    "    lines.append(\"TIMINGS (seconds)\")\n",
    "    total=sum(timings.values())\n",
    "    for k, v in timings.items():\n",
    "        lines.append(f\"{k:<45}: {v:.3f}\")\n",
    "    lines.append(f\"Total: {total:.3f}\")\n",
    "    lines.append(\"\\n=============================================================\\n\")\n",
    "\n",
    "    output_str = \"\\n\".join(lines)\n",
    "\n",
    "    if print_output:\n",
    "        print(output_str)\n",
    "\n",
    "    if save_output:\n",
    "        os.makedirs(\"experiments\", exist_ok=True)\n",
    "        path = os.path.join(\"experiments\", f\"{format_timestamp()}_{experiment_name}.txt\")\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(output_str)\n",
    "\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c6e83",
   "metadata": {},
   "source": [
    "## OpenAI Embedding and Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469ab400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "import h5py, numpy as np\n",
    "\n",
    "def save_embeddings_h5(\n",
    "    path: str,\n",
    "    labels: List[Any],\n",
    "    embeds: List[List[float]],\n",
    "    meta: Optional[Dict] = None\n",
    ") -> None:\n",
    "    E = np.asarray(embeds, dtype=\"float32\")\n",
    "    n, d = E.shape\n",
    "    L = np.asarray([str(l) for l in labels], dtype=h5py.string_dtype(\"utf-8\"))\n",
    "    if len(L) != n:\n",
    "        raise ValueError(\"labels length must match number of embeddings\")\n",
    "\n",
    "    with h5py.File(path, \"a\") as f:\n",
    "        if \"embeddings\" not in f:\n",
    "            f.create_dataset(\"embeddings\", data=E, maxshape=(None, d),\n",
    "                             chunks=True, compression=\"gzip\", compression_opts=4)\n",
    "            f.create_dataset(\"labels\", data=L, maxshape=(None,),\n",
    "                             chunks=True, compression=\"gzip\", compression_opts=4)\n",
    "            if meta:\n",
    "                for k, v in meta.items():\n",
    "                    f.attrs[k] = v\n",
    "        else:\n",
    "            de, dl = f[\"embeddings\"], f[\"labels\"]\n",
    "            if de.shape[1] != d:\n",
    "                raise ValueError(f\"dim mismatch: file {de.shape[1]} vs new {d}\")\n",
    "            start = de.shape[0]\n",
    "            de.resize(start + n, axis=0)\n",
    "            dl.resize(start + n, axis=0)\n",
    "            de[start:start+n] = E\n",
    "            dl[start:start+n] = L\n",
    "\n",
    "def load_embeddings_h5(path: str, n: Optional[int] = None) -> tuple[list[list[float]], list[str]]:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        for name in (\"embeddings\", \"labels\"):\n",
    "            if name not in f:\n",
    "                raise ValueError(f\"File missing '{name}' dataset\")\n",
    "        de, dl = f[\"embeddings\"], f[\"labels\"]\n",
    "        total = de.shape[0]\n",
    "        end = total if n is None else min(total, n)\n",
    "        vectors = de[:end]\n",
    "        labels = dl.asstr()[:end]\n",
    "    return vectors.tolist(), labels.tolist()\n",
    "\n",
    "def get_batch_embeddings(\n",
    "    text_list: list[str], \n",
    "    batch_size: int = 512,\n",
    "    dimensions: int = 128,\n",
    "    model: str = \"text-embedding-3-small\",\n",
    "    show_bar: bool = True\n",
    ") -> list[list[float]]:\n",
    "    client = OpenAI()\n",
    "    embeddings: List[List[float]] = []\n",
    "    texts = text_list\n",
    "    iterator = range(0, len(texts), batch_size)\n",
    "    pbar = tqdm(total=len(texts), unit=\"emb\", desc=\"Embedding\", disable=not show_bar)\n",
    "    try:\n",
    "        for i in iterator:\n",
    "            batch = texts[i:i + batch_size]\n",
    "            resp = client.embeddings.create(input=batch, model=model, dimensions=dimensions)\n",
    "            embeddings.extend([d.embedding for d in resp.data])\n",
    "            pbar.update(len(batch))\n",
    "    finally:\n",
    "        pbar.close()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_batch_embeddings_to_h5(\n",
    "    out_path: str,\n",
    "    text_list: list[str],\n",
    "    labels_list: list[str],\n",
    "    batch_size: int = 512,\n",
    "    dimensions: int = 128,\n",
    "    model: str = \"text-embedding-3-small\",\n",
    "    show_bar: bool = True,\n",
    "    meta: Optional[Dict] = None,\n",
    "    return_data: bool = False,\n",
    "    append_existing: bool = False,\n",
    ") -> Optional[Tuple[list[list[float]], list[str]]]:\n",
    "    if len(text_list) != len(labels_list):\n",
    "        raise ValueError(\"text_list and labels_list must be same length\")\n",
    "\n",
    "    if os.path.exists(out_path) and not append_existing:\n",
    "        print(f\"File '{out_path}' already exists. Skipping embedding generation.\")\n",
    "        return None\n",
    "\n",
    "    client = OpenAI()\n",
    "    ret_E: List[List[float]] = []\n",
    "    ret_L: List[str] = []\n",
    "\n",
    "    iterator = range(0, len(text_list), batch_size)\n",
    "    pbar = tqdm(total=len(text_list), unit=\"emb\", desc=\"Embedding\", disable=not show_bar)\n",
    "\n",
    "    try:\n",
    "        for i in iterator:\n",
    "            batch_t = text_list[i:i + batch_size]\n",
    "            batch_l = labels_list[i:i + batch_size]\n",
    "            resp = client.embeddings.create(input=batch_t, model=model, dimensions=dimensions)\n",
    "            batch_e = [d.embedding for d in resp.data]\n",
    "\n",
    "            save_embeddings_h5(out_path, batch_l, batch_e, meta=meta)\n",
    "\n",
    "            if return_data:\n",
    "                ret_E.extend(batch_e)\n",
    "                ret_L.extend([str(x) for x in batch_l])\n",
    "\n",
    "            pbar.update(len(batch_t))\n",
    "    finally:\n",
    "        pbar.close()\n",
    "\n",
    "    return (ret_E, ret_L) if return_data else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae20a4b",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df99bf8",
   "metadata": {},
   "source": [
    "## Option 1: Custom Semantic Router - Direct Redis access - Allows for trying out techniques with Numpy Arrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e302b7d",
   "metadata": {},
   "source": [
    "### Select and Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ef6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from datasets import load_dataset\n",
    "\n",
    "# See dataset page @ https://huggingface.co/datasets/ + repo_id\n",
    "eval_datasets={\n",
    "                \"Topic\": {\n",
    "                    \"DBLP\":{\n",
    "                        \"repo_id\": \"waashk/dblp\",\n",
    "                        \"train_lbl\": \"train\",\n",
    "                        \"test_lbl\": \"test\",\n",
    "                        \"text_col\": \"text\",\n",
    "                        \"label_col\": \"label\"\n",
    "                      }\n",
    "                    },\n",
    "                \"Sentiment\": {\n",
    "                    \"MPQA\":{\n",
    "                        \"repo_id\": \"jxm/mpqa\",\n",
    "                        \"train_lbl\": \"train\",\n",
    "                        \"test_lbl\": \"test\",\n",
    "                        \"text_col\": \"sentence\",\n",
    "                        \"label_col\": \"label\"\n",
    "                    }\n",
    "                }\n",
    "              }\n",
    "\n",
    "router_config = {\n",
    "    \"name\": \"test\",\n",
    "    \"batch_size\": 1024,\n",
    "    \"vectorizer_config\":{\n",
    "        \"dimensions\":128,\n",
    "        \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "        \"OPENAI_MODEL\": \"text-embedding-3-small\"\n",
    "    },\n",
    "    \"dataset_domain\": \"Sentiment\",\n",
    "    \"dataset_name\": \"MPQA\",\n",
    "}\n",
    "\n",
    "redis_schema = {\n",
    "    \"index\": {\n",
    "        \"name\": \"MPQA\",\n",
    "        \"prefix\": \"test1\",\n",
    "        \"storage_type\": \"hash\",\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"label\", \"type\": \"tag\"},\n",
    "        {\n",
    "            \"name\": \"vector\",\n",
    "            \"type\": \"vector\",\n",
    "            \"attrs\": {\n",
    "                \"dims\": router_config[\"vectorizer_config\"][\"dimensions\"],\n",
    "                \"distance_metric\": \"cosine\",\n",
    "                \"algorithm\": \"hnsw\",\n",
    "                \"datatype\": \"float32\"\n",
    "            }\n",
    "\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "experiment_notes = \"\"\n",
    "\n",
    "timer=Timer()\n",
    "\n",
    "with timer.timeit(\"load_dataset\"):\n",
    "    dataset_repo_id = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"repo_id\"]\n",
    "    dataset_train_lbl = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"train_lbl\"]\n",
    "    dataset_test_lbl = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"test_lbl\"]\n",
    "    dataset_text_col = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"text_col\"]\n",
    "    dataset_label_col = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"label_col\"]\n",
    "\n",
    "    dataset = load_dataset(dataset_repo_id)\n",
    "    train_data = dataset[dataset_train_lbl]\n",
    "    test_data = dataset[dataset_test_lbl]\n",
    "\n",
    "    router_config.update({\"dataset_repo_id\":dataset_repo_id})\n",
    "    router_config.update({\"train_limit\":train_data.num_rows})\n",
    "    router_config.update({\"test_limit\":test_data.num_rows})\n",
    "\n",
    "# Define Experiment Names\n",
    "safe_repo = dataset_repo_id.replace(\"/\", \"-\")\n",
    "experiment_name = f'{router_config[\"name\"]}_{safe_repo}_{router_config[\"vectorizer_config\"][\"OPENAI_MODEL\"]}_{router_config[\"vectorizer_config\"][\"dimensions\"]}'\n",
    "train_filepath = f'embeddings/{experiment_name}_{dataset_train_lbl}_{str(train_data.num_rows)}.h5'\n",
    "test_filepath  = f'embeddings/{experiment_name}_{dataset_test_lbl}_{str(test_data.num_rows)}.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49e598",
   "metadata": {},
   "source": [
    "### Pre-Generate Embeddings for Train and Test Data, Save to Embeddings Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a212b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'embeddings/test_jxm-mpqa_text-embedding-3-small_128_train_8603.h5' already exists. Skipping embedding generation.\n",
      "File 'embeddings/test_jxm-mpqa_text-embedding-3-small_128_test_2000.h5' already exists. Skipping embedding generation.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"embeddings\", exist_ok=True)\n",
    "\n",
    "with timer.timeit(\"Embed and Save Train Data\"):\n",
    "    train_embeddings = get_batch_embeddings_to_h5(out_path=train_filepath,\n",
    "                                                text_list=train_data[dataset_text_col],\n",
    "                                                labels_list=train_data[dataset_label_col],\n",
    "                                                batch_size=router_config[\"batch_size\"],\n",
    "                                                dimensions=router_config[\"vectorizer_config\"][\"dimensions\"],\n",
    "                                                model=router_config[\"vectorizer_config\"][\"OPENAI_MODEL\"])\n",
    "\n",
    "with timer.timeit(\"Embed and Save Test Data\"):\n",
    "    test_embeddings = get_batch_embeddings_to_h5(out_path=test_filepath,\n",
    "                                                text_list=test_data[dataset_text_col],\n",
    "                                                labels_list=test_data[dataset_label_col],\n",
    "                                                batch_size=router_config[\"batch_size\"],\n",
    "                                                dimensions=router_config[\"vectorizer_config\"][\"dimensions\"],\n",
    "                                                model=router_config[\"vectorizer_config\"][\"OPENAI_MODEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19502e9b",
   "metadata": {},
   "source": [
    "### Load and Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1ff508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import redis\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.index import SearchIndex\n",
    "\n",
    "def _kmeans(x, k, iters=20, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    k = min(k, len(x))\n",
    "    idx = rng.choice(len(x), size=k, replace=False)\n",
    "    c = x[idx].copy()\n",
    "    for _ in range(iters):\n",
    "        d = ((x[:, None, :] - c[None, :, :])**2).sum(-1)\n",
    "        a = d.argmin(1)\n",
    "        new_c = []\n",
    "        for j in range(k):\n",
    "            sj = (a == j)\n",
    "            if sj.any():\n",
    "                new_c.append(x[sj].mean(0))\n",
    "            else:\n",
    "                new_c.append(c[rng.integers(0, k)])\n",
    "        new_c = np.stack(new_c, 0)\n",
    "        if np.allclose(new_c, c):\n",
    "            break\n",
    "        c = new_c\n",
    "    return c, a\n",
    "\n",
    "def build_multi_prototypes(vectors, labels, mask=None, k_per_class=1, \n",
    "                           max_points_per_proto=None, normalize=True, seed=0):\n",
    "    \"\"\"\n",
    "    If max_points_per_proto is set, k for class c becomes ceil(n_c / max_points_per_proto).\n",
    "    Otherwise use fixed k_per_class.\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels)\n",
    "    V = np.asarray(vectors, dtype=np.float32)\n",
    "    if mask is None:\n",
    "        mask = np.linalg.norm(V, axis=1) > 0\n",
    "    V, L = V[mask], labels[mask]\n",
    "\n",
    "    out = []\n",
    "    for cls in np.unique(L):\n",
    "        sel = (L == cls)\n",
    "        X = V[sel]\n",
    "        if len(X) == 0:\n",
    "            continue\n",
    "\n",
    "        if max_points_per_proto is not None and max_points_per_proto > 0:\n",
    "            k = int(np.ceil(len(X) / max_points_per_proto))\n",
    "        else:\n",
    "            k = int(k_per_class)\n",
    "        k = max(1, min(k, len(X)))\n",
    "\n",
    "        centers, assign = _kmeans(X, k=k, seed=seed)\n",
    "        for j in range(len(centers)):\n",
    "            sj = (assign == j)\n",
    "            if not sj.any():\n",
    "                continue\n",
    "            mu = centers[j]\n",
    "            if normalize:\n",
    "                n = np.linalg.norm(mu)\n",
    "                if n > 0:\n",
    "                    mu = mu / (n + 1e-12)\n",
    "            weight = float(sj.mean())  # fraction of class points in this prototype\n",
    "            out.append({\"vector\": mu.astype(np.float32).tobytes(), \"label\": cls, \"weight\": weight})\n",
    "    return out\n",
    "\n",
    "def route_label(vec, labels, index, k=5):\n",
    "    means = {}\n",
    "    for lab in labels:\n",
    "        q = VectorQuery(\n",
    "            vector=list(map(float, vec)),\n",
    "            vector_field_name=\"vector\",\n",
    "            num_results=k,\n",
    "            return_fields=[\"label\", \"vector_distance\"],\n",
    "            filter_expression=f\"@label:{{{lab}}}\",\n",
    "        )\n",
    "        hits = index.query(q)\n",
    "        if hits:\n",
    "            dists = [float(h[\"vector_distance\"]) for h in hits]\n",
    "            means[lab] = sum(dists) / len(dists)\n",
    "    if not means:\n",
    "        return None, {}\n",
    "    best = min(means, key=means.get)\n",
    "    return best, means\n",
    "\n",
    "def route_batch(vectors, labels, index, k=5):\n",
    "    out = []\n",
    "    for v in tqdm(vectors):\n",
    "        best, _ = route_label(v, labels, index, k)\n",
    "        out.append(best)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfcfcd7",
   "metadata": {},
   "source": [
    "### Pre-Generated Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b10c815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "with timer.timeit(\"Load and process train data, create Redis Index\"):\n",
    "    r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB)\n",
    "    try:\n",
    "        r.flushdb()\n",
    "        r.ft(redis_schema[\"index\"][\"name\"]).dropindex() \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    index = SearchIndex.from_dict(redis_schema, redis_url=REDIS_URL)\n",
    "    index.create(overwrite=True)\n",
    "    vectors,labels=load_embeddings_h5(train_filepath)\n",
    "    unique_labels=list(set(labels))\n",
    "    vectors = np.array(vectors)\n",
    "    data = build_multi_prototypes(vectors, labels, mask=None, k_per_class=5, max_points_per_proto=None, normalize=True, seed=0)\n",
    "\n",
    "    keys = index.load(data)\n",
    "index.info()['num_docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce3e5d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 2039.42it/s]\n"
     ]
    }
   ],
   "source": [
    "with timer.timeit(\"Load and process test data\"):\n",
    "    test_vectors,test_labels=load_embeddings_h5(test_filepath, n=test_data.num_rows)\n",
    "    test_vectors = np.array(test_vectors)\n",
    "    test_vectors /= np.linalg.norm(test_vectors, axis=1, keepdims=True) + 1e-12\n",
    "with timer.timeit(\"Route test data\"):\n",
    "    preds=route_batch(test_vectors, unique_labels, index, k=5)\n",
    "\n",
    "eval_metrics = classification_metrics(\n",
    "    preds=preds,\n",
    "    labels=test_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11190f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment Summary: Semantic Routing Classification ===\n",
      "\n",
      "Timestamp         : 2025_10_15_164430\n",
      "\n",
      "DATA CONFIGURATION\n",
      "Router Name       : test\n",
      "Batch Size        : 1024\n",
      "Dataset Domain    : Sentiment\n",
      "Dataset Name      : MPQA\n",
      "Train Limit       : 8603\n",
      "Test Limit        : 2000\n",
      "Vector Dimensions : 128\n",
      "Embedding Model   : text-embedding-3-small\n",
      "\n",
      "REDIS SCHEMA\n",
      "Index Name        : MPQA\n",
      "Prefix            : test1\n",
      "Storage Type      : hash\n",
      "Fields:\n",
      "  - label (tag)\n",
      "  - vector (vector)  dims=128, metric=cosine, algo=hnsw\n",
      "\n",
      "REDIS INDEX INFO\n",
      "Documents         : 10\n",
      "Max Doc ID        : 10\n",
      "Num Terms         : 0\n",
      "Num Records       : 20\n",
      "Inverted Size (MB): 0.000164\n",
      "Vector Index (MB) : 0.724480\n",
      "\n",
      "EVALUATION METRICS\n",
      "Accuracy          : 0.8360\n",
      "Precision (macro) : 0.8361\n",
      "Recall (macro)    : 0.8360\n",
      "F1 (macro)        : 0.8360\n",
      "\n",
      "CONFUSION MATRIX [ [TP_0, FP_0], [FN_1, TP_1] ]\n",
      "  [827, 173]\n",
      "  [155, 845]\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "Class 0: Precision=0.842, Recall=0.827, F1=0.835, Support=1000\n",
      "Class 1: Precision=0.830, Recall=0.845, F1=0.837, Support=1000\n",
      "\n",
      "Macro Avg:\n",
      "  Precision=0.836, Recall=0.836, F1=0.836\n",
      "\n",
      "TIMINGS (seconds)\n",
      "load_dataset                                 : 2.126\n",
      "Embed and Save Train Data                    : 0.001\n",
      "Embed and Save Test Data                     : 0.000\n",
      "Load and process train data, create Redis Index: 0.334\n",
      "Load and process test data                   : 0.019\n",
      "Route test data                              : 0.982\n",
      "\n",
      "=============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'=== Experiment Summary: Semantic Routing Classification ===\\n\\nTimestamp         : 2025_10_15_164430\\n\\nDATA CONFIGURATION\\nRouter Name       : test\\nBatch Size        : 1024\\nDataset Domain    : Sentiment\\nDataset Name      : MPQA\\nTrain Limit       : 8603\\nTest Limit        : 2000\\nVector Dimensions : 128\\nEmbedding Model   : text-embedding-3-small\\n\\nREDIS SCHEMA\\nIndex Name        : MPQA\\nPrefix            : test1\\nStorage Type      : hash\\nFields:\\n  - label (tag)\\n  - vector (vector)  dims=128, metric=cosine, algo=hnsw\\n\\nREDIS INDEX INFO\\nDocuments         : 10\\nMax Doc ID        : 10\\nNum Terms         : 0\\nNum Records       : 20\\nInverted Size (MB): 0.000164\\nVector Index (MB) : 0.724480\\n\\nEVALUATION METRICS\\nAccuracy          : 0.8360\\nPrecision (macro) : 0.8361\\nRecall (macro)    : 0.8360\\nF1 (macro)        : 0.8360\\n\\nCONFUSION MATRIX [ [TP_0, FP_0], [FN_1, TP_1] ]\\n  [827, 173]\\n  [155, 845]\\n\\nCLASSIFICATION REPORT\\nClass 0: Precision=0.842, Recall=0.827, F1=0.835, Support=1000\\nClass 1: Precision=0.830, Recall=0.845, F1=0.837, Support=1000\\n\\nMacro Avg:\\n  Precision=0.836, Recall=0.836, F1=0.836\\n\\nTIMINGS (seconds)\\nload_dataset                                 : 2.126\\nEmbed and Save Train Data                    : 0.001\\nEmbed and Save Test Data                     : 0.000\\nLoad and process train data, create Redis Index: 0.334\\nLoad and process test data                   : 0.019\\nRoute test data                              : 0.982\\n\\n=============================================================\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_summary = print_experiment_summary(eval_metrics, redis_schema, router_config, timer.timings, index.info(), experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f860c1",
   "metadata": {},
   "source": [
    "# Option 2: OOTB Semantic Routing - The Offering from RedisVL, slight changes to handle more data bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b573c",
   "metadata": {},
   "source": [
    "### Custom Semantic Router - Allows messing with dimensions + batch sizes for bulky preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bfb00228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tenacity.retry import retry_if_not_exception_type\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from redisvl.extensions.router import SemanticRouter\n",
    "from redisvl.extensions.router import Route\n",
    "from redisvl.utils.vectorize import OpenAITextVectorizer\n",
    "from redisvl.extensions.cache.embeddings.embeddings import EmbeddingsCache\n",
    "from datasets import load_dataset, Dataset\n",
    "from redisvl.extensions.router.schema import (\n",
    "    DistanceAggregationMethod,\n",
    "    Route,\n",
    "    RouteMatch\n",
    ")\n",
    "from redis.commands.search.aggregation import AggregateRequest, AggregateResult, Reducer\n",
    "from redisvl.redis.utils import hashify\n",
    "from redisvl.query import FilterQuery, VectorRangeQuery\n",
    "from redisvl.extensions.constants import ROUTE_VECTOR_FIELD_NAME\n",
    "from redis.exceptions import ResponseError\n",
    "\n",
    "\n",
    "\n",
    "def build_route(\n",
    "    references: list[str],\n",
    "    label: str,\n",
    "    priority: int = 1,\n",
    "    distance_threshold: float = 0.5\n",
    "    ) -> Route:\n",
    "  route = Route(\n",
    "      name=label,\n",
    "      references=references,\n",
    "      metadata={\"category\": label, \"priority\": priority},\n",
    "      distance_threshold=distance_threshold\n",
    "  )\n",
    "  return route\n",
    "\n",
    "def build_routes_from_hf_data(\n",
    "    dataset: Dataset,\n",
    "    text_col: str = \"text\",\n",
    "    label_col: str = \"label\",\n",
    "    distance_threshold: float = 0.5\n",
    "    ) -> list[Route]:\n",
    "    '''\n",
    "    Expects dataset to be of form:\n",
    "    Dataset({\n",
    "        features: ['text', 'label'],\n",
    "        num_rows: 343152\n",
    "    })\n",
    "    '''\n",
    "    df = dataset.to_pandas()[[text_col, label_col]]\n",
    "    texts_by_label = df.groupby(label_col)[text_col].apply(list).to_dict()\n",
    "    return [build_route(references=refs, label=str(lbl), distance_threshold=distance_threshold) for lbl, refs in texts_by_label.items()]\n",
    "\n",
    "\n",
    "class HighVisOpenAITextVectorizer(OpenAITextVectorizer):\n",
    "    @retry(\n",
    "    wait=wait_random_exponential(min=1, max=60),\n",
    "    stop=stop_after_attempt(6),\n",
    "    retry=retry_if_not_exception_type(TypeError),\n",
    "    )\n",
    "    def _embed_many(\n",
    "        self, texts: List[str], batch_size: int = 1024, dimensions: int = 1536, **kwargs\n",
    "    ) -> List[List[float]]:\n",
    "        \"\"\"Exactly the same but with progress bar\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            raise TypeError(\"Must pass in a list of str values to embed.\")\n",
    "        if texts and not isinstance(texts[0], str):\n",
    "            raise TypeError(\"Must pass in a list of str values to embed.\")\n",
    "\n",
    "        embeddings: List = []\n",
    "\n",
    "        pbar = tqdm(total=len(texts), unit=\"emb\", desc=\"Embedding\", disable=False)\n",
    "        for batch in tqdm(self.batchify(texts, batch_size)):\n",
    "            try:\n",
    "                response = self._client.embeddings.create(\n",
    "                    input=batch, model=self.model, dimensions=1536, **kwargs\n",
    "                )\n",
    "                embeddings += [r.embedding for r in response.data]\n",
    "                pbar.update(len(batch))\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Embedding texts failed: {e}\")\n",
    "        pbar.close()\n",
    "        return embeddings\n",
    "\n",
    "class BigBatchSemanticRouter(SemanticRouter):\n",
    "    def _add_routes(self, routes: List[Route]):\n",
    "        \"\"\"Add routes to the router and index.\n",
    "\n",
    "        Args:\n",
    "            routes (List[Route]): List of routes to be added.\n",
    "        \"\"\"\n",
    "        route_references: List[Dict[str, Any]] = []\n",
    "        keys: List[str] = []\n",
    "\n",
    "        for route in routes:\n",
    "            # embed route references as a single batch\n",
    "            reference_vectors = self.vectorizer.embed_many(\n",
    "                [reference for reference in route.references], as_buffer=True, batch_size=1024, dimensions=1536\n",
    "            )\n",
    "            # set route references\n",
    "            for i, reference in enumerate(route.references):\n",
    "                reference_hash = hashify(reference)\n",
    "                route_references.append(\n",
    "                    {\n",
    "                        \"reference_id\": reference_hash,\n",
    "                        \"route_name\": route.name,\n",
    "                        \"reference\": reference,\n",
    "                        \"vector\": reference_vectors[i],\n",
    "                    }\n",
    "                )\n",
    "                keys.append(\n",
    "                    self._route_ref_key(self._index, route.name, reference_hash)\n",
    "                )\n",
    "            # set route if does not yet exist client side\n",
    "            if not self.get(route.name):\n",
    "                self.routes.append(route)\n",
    "        self._index.load(route_references, keys=keys)\n",
    "\n",
    "    def _get_route_matches(\n",
    "        self,\n",
    "        vector: List[float],\n",
    "        aggregation_method: DistanceAggregationMethod,\n",
    "        max_k: int = 1,\n",
    "    ) -> List[RouteMatch]:\n",
    "        \"\"\"Get route response from vector db\"\"\"\n",
    "\n",
    "        # what's interesting about this is that we only provide one distance_threshold for a range query not multiple\n",
    "        # therefore you might take the max_threshold and further refine from there.\n",
    "        distance_threshold = max(route.distance_threshold for route in self.routes)\n",
    "\n",
    "        vector_range_query = VectorRangeQuery(\n",
    "            vector=vector,\n",
    "            vector_field_name=ROUTE_VECTOR_FIELD_NAME,\n",
    "            distance_threshold=float(distance_threshold),\n",
    "            return_fields=[\"route_name\"],\n",
    "        )\n",
    "\n",
    "        aggregate_request = self._build_aggregate_request(\n",
    "            vector_range_query, aggregation_method, max_k=max_k\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            aggregation_result: AggregateResult = self._index.aggregate(\n",
    "                aggregate_request, vector_range_query.params\n",
    "            )\n",
    "        except ResponseError as e:\n",
    "            if \"VSS is not yet supported on FT.AGGREGATE\" in str(e):\n",
    "                raise RuntimeError(\n",
    "                    \"Semantic routing is only available on Redis version 7.x.x or greater\"\n",
    "                )\n",
    "            raise e\n",
    "\n",
    "        # process aggregation results into route matches\n",
    "        return [\n",
    "            self._process_route(route_match) for route_match in aggregation_result.rows\n",
    "        ]\n",
    "    \n",
    "    def _classify_multi_route(\n",
    "        self,\n",
    "        vector: List[float],\n",
    "        max_k: int,\n",
    "        aggregation_method: DistanceAggregationMethod,\n",
    "    ) -> List[RouteMatch]:\n",
    "        \"\"\"Classify to multiple routes, up to max_k (int), using a vector.\"\"\"\n",
    "\n",
    "        route_matches = self._get_route_matches(vector, aggregation_method, max_k=max_k)\n",
    "\n",
    "        # process route matches\n",
    "        top_route_matches: List[RouteMatch] = []\n",
    "        if route_matches:\n",
    "            for route_match in route_matches:\n",
    "                if route_match.name is not None:\n",
    "                    top_route_matches.append(route_match)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"{route_match.name} not a supported route for the {self.name} semantic router.\"\n",
    "                    )\n",
    "\n",
    "        return top_route_matches\n",
    "\n",
    "    def bulk_route(\n",
    "        self,\n",
    "        statements: Optional[List[str]] = None,\n",
    "        vectors: Optional[List[List[float]]] = None,\n",
    "        max_k: Optional[int] = None,\n",
    "        distance_threshold: Optional[float] = None,\n",
    "        aggregation_method: Optional[DistanceAggregationMethod] = None,\n",
    "    ) -> List[List[RouteMatch]]:\n",
    "        \"\"\"For mass routing.\n",
    "        \"\"\"\n",
    "        if not vectors:\n",
    "            if not statements:\n",
    "                raise ValueError(\"Must provide a list of vectors or statements to the router\")\n",
    "            vectors = self.vectorizer.embed_many(statements, batch_size=1024, dimensions = 1536)  # type: ignore\n",
    "\n",
    "        max_k = max_k or self.routing_config.max_k\n",
    "        aggregation_method = (\n",
    "            aggregation_method or self.routing_config.aggregation_method\n",
    "        )\n",
    "\n",
    "        pbar = tqdm(total=len(vectors), unit=\"pred\", desc=\"Assessing\", disable=False)\n",
    "        results: List[List[RouteMatch]] = []\n",
    "        for v in vectors:\n",
    "            matches = self._classify_multi_route(v, max_k, aggregation_method)  # type: ignore\n",
    "            if distance_threshold is not None:\n",
    "                try:\n",
    "                    matches = [m for m in matches if m.distance <= distance_threshold]  # type: ignore\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "            results.append(matches)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        return results\n",
    "\n",
    "def nuke_redis():\n",
    "    import redis\n",
    "    try:\n",
    "        r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB)\n",
    "        r.flushdb()\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f61b81",
   "metadata": {},
   "source": [
    "### Custom Router Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59838920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from redisvl.extensions.router.semantic import SemanticRouter\n",
    "\n",
    "from redis_retrieval_optimizer.threshold_optimization.base import (\n",
    "    BaseThresholdOptimizer,\n",
    "    EvalMetric,\n",
    ")\n",
    "from redis_retrieval_optimizer.threshold_optimization.schema import LabeledData\n",
    "from redis_retrieval_optimizer.threshold_optimization.utils import (\n",
    "    NULL_RESPONSE_KEY,\n",
    "    _format_qrels,\n",
    ")\n",
    "\n",
    "\n",
    "def _generate_run_router(test_data: List[LabeledData], router: SemanticRouter) -> \"Run\":\n",
    "    \"\"\"Format router results into format for ranx Run\"\"\"\n",
    "    if Run is None:\n",
    "        raise ImportError(\"ranx is required for threshold optimization\")\n",
    "    if np is None:\n",
    "        raise ImportError(\"numpy is required for threshold optimization\")\n",
    "\n",
    "    run_dict: Dict[Any, Any] = {}\n",
    "\n",
    "    bulk_route = router.bulk_route(statements=[td.query for td in test_data], max_k=1)\n",
    "    print(len(bulk_route))\n",
    "    for td, match in zip(test_data, bulk_route):\n",
    "        run_dict[td.id] = {}\n",
    "        if match:\n",
    "            run_dict[td.id][match[0].name] = np.int64(1)\n",
    "        else:\n",
    "            run_dict[td.id][NULL_RESPONSE_KEY] = np.int64(1)\n",
    "\n",
    "    # for td in test_data:\n",
    "    #     run_dict[td.id] = {}\n",
    "    #     route_match = router(td.query)\n",
    "    #     if route_match and route_match.name == td.query_match:\n",
    "    #         run_dict[td.id][td.query_match] = np.int64(1)\n",
    "    #     else:\n",
    "    #         run_dict[td.id][NULL_RESPONSE_KEY] = np.int64(1)\n",
    "\n",
    "    return Run(run_dict)\n",
    "\n",
    "\n",
    "def _eval_router(\n",
    "    router: SemanticRouter,\n",
    "    test_data: List[LabeledData],\n",
    "    qrels: \"Qrels\",\n",
    "    eval_metric: str,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate acceptable metric given run and qrels data\"\"\"\n",
    "    if evaluate is None:\n",
    "        raise ImportError(\"ranx is required for threshold optimization\")\n",
    "\n",
    "    run = _generate_run_router(test_data, router)\n",
    "    return evaluate(qrels, run, eval_metric, make_comparable=True)\n",
    "\n",
    "\n",
    "def _router_random_search(\n",
    "    route_names: List[str], route_thresholds: dict, search_step=0.10\n",
    "):\n",
    "    \"\"\"Performs random search for many thresholds to many routes\"\"\"\n",
    "    if np is None:\n",
    "        raise ImportError(\"numpy is required for threshold optimization\")\n",
    "\n",
    "    score_threshold_values = []\n",
    "    for route in route_names:\n",
    "        score_threshold_values.append(\n",
    "            np.linspace(\n",
    "                start=max(route_thresholds[route] - search_step, 0),\n",
    "                stop=route_thresholds[route] + search_step,\n",
    "                num=100,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        route: float(random.choice(score_threshold_values[i]))\n",
    "        for i, route in enumerate(route_names)\n",
    "    }\n",
    "\n",
    "\n",
    "def _random_search_opt_router(\n",
    "    router: SemanticRouter,\n",
    "    test_data: List[LabeledData],\n",
    "    qrels: \"Qrels\",\n",
    "    eval_metric: EvalMetric,\n",
    "    **kwargs: Any,\n",
    "):\n",
    "    \"\"\"Performs complete optimization for router cases provide acceptable metric\"\"\"\n",
    "    print(\"Starting Optimization\")\n",
    "\n",
    "    start_score = _eval_router(router, test_data, qrels, eval_metric.value)\n",
    "    best_score = start_score\n",
    "    best_thresholds = router.route_thresholds\n",
    "\n",
    "    max_iterations = kwargs.get(\"max_iterations\", 20)\n",
    "    search_step = kwargs.get(\"search_step\", 0.10)\n",
    "\n",
    "    pbar = tqdm(total=max_iterations, desc=\"Optimizing Routes\")\n",
    "    for _ in range(max_iterations):\n",
    "        route_names = router.route_names\n",
    "        route_thresholds = router.route_thresholds\n",
    "        thresholds = _router_random_search(\n",
    "            route_names=route_names,\n",
    "            route_thresholds=route_thresholds,\n",
    "            search_step=search_step,\n",
    "        )\n",
    "        router.update_route_thresholds(thresholds)\n",
    "        print(\"Eval starting\")\n",
    "        score = _eval_router(router, test_data, qrels, eval_metric.value)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresholds = thresholds\n",
    "        pbar.update(1)\n",
    "\n",
    "    print(\n",
    "        f\"Eval metric {eval_metric.value.upper()}: start {round(start_score, 3)}, end {round(best_score, 3)} \\nEnding thresholds: {router.route_thresholds}\"\n",
    "    )\n",
    "    router.update_route_thresholds(best_thresholds)\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "class BulkRouterThresholdOptimizer(BaseThresholdOptimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        router: SemanticRouter,\n",
    "        test_dict: List[Dict[str, Any]],\n",
    "        opt_fn: Callable = _random_search_opt_router,\n",
    "        eval_metric: str = \"f1\",\n",
    "    ):\n",
    "        \"\"\"Initialize the router optimizer.\n",
    "\n",
    "        Args:\n",
    "            router (SemanticRouter): The RedisVL SemanticRouter instance to optimize.\n",
    "            test_dict (List[Dict[str, Any]]): List of test cases.\n",
    "            opt_fn (Callable): Function to perform optimization. Defaults to\n",
    "                grid search.\n",
    "            eval_metric (str): Evaluation metric for threshold optimization.\n",
    "                Defaults to \"f1\" score.\n",
    "        Raises:\n",
    "            ValueError: If the test_dict not in LabeledData format.\n",
    "        \"\"\"\n",
    "        super().__init__(router, test_dict, opt_fn, eval_metric)\n",
    "\n",
    "    def optimize(self, **kwargs: Any):\n",
    "        \"\"\"Optimize kicks off the optimization process for router\"\"\"\n",
    "        qrels = _format_qrels(self.test_data)\n",
    "        self.opt_fn(self.optimizable, self.test_data, qrels, self.eval_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea4cad",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c33243",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuke_redis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52e86d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:51:35 huggingface_hub.repocard WARNING   Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "eval_datasets={\n",
    "                \"Topic\": {\n",
    "                    \"DBLP\":{\n",
    "                        \"repo_id\": \"waashk/dblp\",\n",
    "                        \"train_lbl\": \"train\",\n",
    "                        \"test_lbl\": \"test\",\n",
    "                        \"text_col\": \"text\",\n",
    "                        \"label_col\": \"label\"\n",
    "                      }\n",
    "                    },\n",
    "                \"Sentiment\": {\n",
    "                    \"SST-2\":{\n",
    "                        \"repo_id\": \"stanfordnlp/sst2\",\n",
    "                        \"train_lbl\": \"train\",\n",
    "                        \"test_lbl\": \"test\",\n",
    "                        \"text_col\": \"sentence\",\n",
    "                        \"label_col\": \"label\",\n",
    "                        \"alternate_scoring_source\": {\n",
    "                            \"repo_id\": \"SetFit/sst2\",\n",
    "                            \"train_lbl\": \"train\",\n",
    "                            \"test_lbl\": \"test\",\n",
    "                            \"text_col\": \"text\",\n",
    "                            \"label_col\": \"label\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"MPQA\":{\n",
    "                        \"repo_id\": \"jxm/mpqa\",\n",
    "                        \"train_lbl\": \"train\",\n",
    "                        \"test_lbl\": \"test\",\n",
    "                        \"text_col\": \"sentence\",\n",
    "                        \"label_col\": \"label\"\n",
    "                    }\n",
    "                }\n",
    "              }\n",
    "\n",
    "router_config = {\n",
    "    \"name\": \"OOTB\",\n",
    "    \"batch_size\": 1024,\n",
    "    \"vectorizer_config\":{\n",
    "        \"dimensions\":1536,\n",
    "        \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "        \"OPENAI_MODEL\": \"text-embedding-3-small\"\n",
    "    },\n",
    "    \"dataset_domain\": \"Sentiment\",\n",
    "    \"dataset_name\": \"SST-2\",\n",
    "}\n",
    "\n",
    "# experiment_notes = \"\"\n",
    "# dataset_repo_id = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"repo_id\"]\n",
    "# dataset = load_dataset(dataset_repo_id)\n",
    "\n",
    "timer=Timer()\n",
    "\n",
    "with timer.timeit(\"load_dataset\"):\n",
    "    dataset_repo_id = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"repo_id\"]\n",
    "    dataset_train_lbl = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"train_lbl\"]\n",
    "    dataset_test_lbl = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"test_lbl\"]\n",
    "    dataset_text_col = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"text_col\"]\n",
    "    dataset_label_col = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"label_col\"]\n",
    "\n",
    "    dataset = load_dataset(dataset_repo_id)\n",
    "\n",
    "    # Merge dev with train\n",
    "    all_splits = list(dataset.keys())\n",
    "    extra_splits = [s for s in all_splits if s not in (dataset_train_lbl, dataset_test_lbl)]\n",
    "    if extra_splits:\n",
    "        train_data = concatenate_datasets([dataset[dataset_train_lbl]] + [dataset[s] for s in extra_splits])\n",
    "    else:\n",
    "        train_data = dataset[dataset_train_lbl][dataset_text_col]\n",
    "\n",
    "    if \"alternate_scoring_source\" in eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]]:\n",
    "        altdataset_repo_id = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"alternate_scoring_source\"][\"repo_id\"]\n",
    "        altdataset_test_lbl = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"alternate_scoring_source\"][\"test_lbl\"]\n",
    "        altdataset = load_dataset(altdataset_repo_id)\n",
    "        test_text_col = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"alternate_scoring_source\"][\"text_col\"]\n",
    "        test_label_col = eval_datasets[router_config[\"dataset_domain\"]][router_config[\"dataset_name\"]][\"alternate_scoring_source\"][\"label_col\"]\n",
    "        test_data = altdataset[altdataset_test_lbl]\n",
    "    else:\n",
    "        test_text_col = dataset_text_col\n",
    "        test_label_col = dataset_label_col\n",
    "        test_data = dataset[dataset_test_lbl]\n",
    "\n",
    "    router_config.update({\"dataset_repo_id\":dataset_repo_id})\n",
    "    router_config.update({\"train_limit\":train_data.num_rows})\n",
    "    router_config.update({\"test_limit\":test_data.num_rows})\n",
    "\n",
    "# Define Experiment Names\n",
    "safe_repo = dataset_repo_id.replace(\"/\", \"-\")\n",
    "experiment_name = f'{router_config[\"name\"]}_{safe_repo}_{router_config[\"vectorizer_config\"][\"OPENAI_MODEL\"]}_{router_config[\"vectorizer_config\"][\"dimensions\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ef5bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# altdataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a26b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'sentence', 'label'],\n",
      "    num_rows: 68221\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 1821\n",
      "})\n",
      "Column([0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)\n",
    "print(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc25d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "20:28:29 redisvl.index.index INFO   Index already exists, overwriting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|          | 0/30208 [00:00<?, ?emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:31 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   3%|▎         | 1024/30208 [00:03<01:42, 284.31emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:35 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   7%|▋         | 2048/30208 [00:06<01:28, 319.27emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:38 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  10%|█         | 3072/30208 [00:09<01:28, 308.24emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:41 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  14%|█▎        | 4096/30208 [00:13<01:23, 314.25emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:44 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  17%|█▋        | 5120/30208 [00:16<01:19, 316.58emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:47 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  20%|██        | 6144/30208 [00:19<01:16, 316.10emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:51 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  24%|██▎       | 7168/30208 [00:23<01:15, 305.26emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:54 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  27%|██▋       | 8192/30208 [00:25<01:06, 331.22emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:28:57 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  31%|███       | 9216/30208 [00:28<01:04, 326.92emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:00 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  34%|███▍      | 10240/30208 [00:31<00:57, 345.26emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:03 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  37%|███▋      | 11264/30208 [00:35<00:59, 320.87emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:06 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  41%|████      | 12288/30208 [00:36<00:47, 375.79emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:08 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  44%|████▍     | 13312/30208 [00:39<00:45, 372.72emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:11 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  47%|████▋     | 14336/30208 [00:42<00:41, 383.09emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:13 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  51%|█████     | 15360/30208 [00:44<00:38, 389.90emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:16 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  54%|█████▍    | 16384/30208 [00:47<00:36, 380.35emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:18 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  58%|█████▊    | 17408/30208 [00:49<00:31, 409.79emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:20 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  61%|██████    | 18432/30208 [00:51<00:26, 446.18emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:23 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  64%|██████▍   | 19456/30208 [00:54<00:27, 396.11emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:26 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  68%|██████▊   | 20480/30208 [00:57<00:26, 371.67emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 21504/30208 [01:00<00:24, 358.38emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:32 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 22528/30208 [01:03<00:21, 356.91emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:34 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  78%|███████▊  | 23552/30208 [01:06<00:19, 349.08emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:38 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████▏ | 24576/30208 [01:10<00:16, 343.48emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:41 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  85%|████████▍ | 25600/30208 [01:13<00:13, 339.61emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:44 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  88%|████████▊ | 26624/30208 [01:15<00:09, 368.92emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  92%|█████████▏| 27648/30208 [01:18<00:07, 357.47emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:50 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  95%|█████████▍| 28672/30208 [01:22<00:04, 326.39emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:53 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  98%|█████████▊| 29696/30208 [01:25<00:01, 310.35emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:57 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [01:28,  2.95s/it]███| 30208/30208 [01:28<00:00, 280.00emb/s]\n",
      "Embedding: 100%|██████████| 30208/30208 [01:28<00:00, 341.32emb/s]\n",
      "Embedding:   0%|          | 0/38013 [00:00<?, ?emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:01 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   3%|▎         | 1024/38013 [00:04<02:25, 253.71emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:04 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   5%|▌         | 2048/38013 [00:06<01:43, 348.13emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:08 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   8%|▊         | 3072/38013 [00:09<01:50, 315.69emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:11 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  11%|█         | 4096/38013 [00:12<01:36, 353.24emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:13 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  13%|█▎        | 5120/38013 [00:14<01:27, 375.32emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:16 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  16%|█▌        | 6144/38013 [00:17<01:23, 383.76emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:18 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  19%|█▉        | 7168/38013 [00:19<01:14, 411.88emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:21 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  22%|██▏       | 8192/38013 [00:23<01:29, 333.59emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:24 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  24%|██▍       | 9216/38013 [00:26<01:25, 337.70emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:27 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  27%|██▋       | 10240/38013 [00:28<01:12, 382.60emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  30%|██▉       | 11264/38013 [00:30<01:03, 421.42emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:31 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  32%|███▏      | 12288/38013 [00:33<01:06, 385.95emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:34 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  35%|███▌      | 13312/38013 [00:36<01:04, 380.05emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:37 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  38%|███▊      | 14336/38013 [00:39<01:06, 353.86emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:40 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  40%|████      | 15360/38013 [00:42<01:05, 347.41emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:43 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  43%|████▎     | 16384/38013 [00:44<00:54, 395.47emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  46%|████▌     | 17408/38013 [00:47<00:52, 393.15emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:48 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  48%|████▊     | 18432/38013 [00:50<00:54, 359.03emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:51 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  51%|█████     | 19456/38013 [00:53<00:52, 351.66emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:54 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  54%|█████▍    | 20480/38013 [00:56<00:49, 352.85emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:57 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  57%|█████▋    | 21504/38013 [00:59<00:46, 352.68emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:00 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  59%|█████▉    | 22528/38013 [01:01<00:38, 400.79emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:02 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  62%|██████▏   | 23552/38013 [01:04<00:40, 358.35emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:05 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  65%|██████▍   | 24576/38013 [01:06<00:33, 401.31emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:07 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  67%|██████▋   | 25600/38013 [01:09<00:31, 392.68emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:10 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  70%|███████   | 26624/38013 [01:12<00:29, 382.58emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:13 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  73%|███████▎  | 27648/38013 [01:14<00:26, 393.89emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:15 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 28672/38013 [01:16<00:22, 420.67emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:18 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  78%|███████▊  | 29696/38013 [01:20<00:22, 369.80emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:21 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████  | 30720/38013 [01:22<00:20, 363.90emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:24 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  84%|████████▎ | 31744/38013 [01:25<00:15, 396.15emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:26 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  86%|████████▌ | 32768/38013 [01:27<00:13, 392.20emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:30 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  89%|████████▉ | 33792/38013 [01:32<00:13, 313.13emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:33 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  92%|█████████▏| 34816/38013 [01:35<00:10, 311.86emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:36 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  94%|█████████▍| 35840/38013 [01:38<00:06, 317.12emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:39 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  97%|█████████▋| 36864/38013 [01:41<00:03, 334.35emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:43 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|█████████▉| 37888/38013 [01:45<00:00, 309.04emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:31:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [01:46,  2.81s/it]███| 38013/38013 [01:46<00:00, 284.88emb/s]\n",
      "Embedding: 100%|██████████| 38013/38013 [01:46<00:00, 356.56emb/s]\n"
     ]
    }
   ],
   "source": [
    "timer=Timer()\n",
    "\n",
    "with timer.timeit(\"Build Routes\"):\n",
    "    routes = build_routes_from_hf_data(dataset=train_data, text_col=dataset_text_col, label_col=dataset_label_col, distance_threshold=0.9)\n",
    "\n",
    "vectorizer=HighVisOpenAITextVectorizer(\n",
    "    model=router_config[\"vectorizer_config\"][\"OPENAI_MODEL\"],\n",
    "    api_config={\"api_key\": router_config[\"vectorizer_config\"][\"OPENAI_API_KEY\"]},\n",
    "    cache=EmbeddingsCache(redis_url=REDIS_URL)\n",
    ")\n",
    "\n",
    "with timer.timeit(\"Build Router\"):\n",
    "    router = BigBatchSemanticRouter(\n",
    "        name=router_config[\"name\"],\n",
    "        routes=routes,\n",
    "        vectorizer=vectorizer,\n",
    "        redis_url=REDIS_URL,\n",
    "        overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ecc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Optional for now\n",
    "'''\n",
    "\n",
    "# train_data_arr=[{\"query\": train_data[dataset_text_col][i], \"query_match\":str(train_data[dataset_label_col][i])} for i in range(len(train_data))]\n",
    "# optimizer = BulkRouterThresholdOptimizer(router, train_data_arr)\n",
    "# optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f437f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|          | 0/1821 [00:00<?, ?emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:51:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  56%|█████▌    | 1024/1821 [00:03<00:03, 264.95emb/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:51:49 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.83s/it]████| 1821/1821 [00:05<00:00, 335.98emb/s]\n",
      "Embedding: 100%|██████████| 1821/1821 [00:05<00:00, 321.36emb/s]\n"
     ]
    }
   ],
   "source": [
    "with timer.timeit(\"Predict using test data\"):\n",
    "    test_statements=[str(i) for i in test_data[test_text_col]]\n",
    "    preds = router.bulk_route(statements=test_statements, max_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bda1700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment Summary: Semantic Routing Classification ===\n",
      "\n",
      "Timestamp         : 2025_10_15_205329\n",
      "\n",
      "Experiment Notes  : NA\n",
      "\n",
      "DATA CONFIGURATION\n",
      "Router Name       : OOTB\n",
      "Batch Size        : 1024\n",
      "Dataset Repo ID    : stanfordnlp/sst2\n",
      "Dataset Domain    : Sentiment\n",
      "Dataset Name      : SST-2\n",
      "Train Limit       : 68221\n",
      "Test Limit        : 1821\n",
      "Vector Dimensions : 1536\n",
      "Embedding Model   : text-embedding-3-small\n",
      "\n",
      "\n",
      "No index info available\n",
      "EVALUATION METRICS\n",
      "Accuracy          : 0.9121\n",
      "Precision (macro) : 0.9138\n",
      "Recall (macro)    : 0.9122\n",
      "F1 (macro)        : 0.9121\n",
      "\n",
      "CONFUSION MATRIX [ [TP_0, FP_0], [FN_1, TP_1] ]\n",
      "  [803, 109]\n",
      "  [51, 858]\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "Class 0: Precision=0.940, Recall=0.880, F1=0.909, Support=912\n",
      "Class 1: Precision=0.887, Recall=0.944, F1=0.915, Support=909\n",
      "\n",
      "Macro Avg:\n",
      "  Precision=0.914, Recall=0.912, F1=0.912\n",
      "\n",
      "TIMINGS (seconds)\n",
      "load_dataset                                 : 10.004\n",
      "Predict using test data                      : 95.192\n",
      "Total: 105.196\n",
      "\n",
      "=============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_preds = [i[0].name for i in preds]\n",
    "labels = [str(i) for i in test_data[test_label_col]]\n",
    "\n",
    "eval_metrics = classification_metrics(\n",
    "    preds=extracted_preds,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "exp_summary = print_experiment_summary(eval_metrics, router_config, timer.timings, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9daaab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
