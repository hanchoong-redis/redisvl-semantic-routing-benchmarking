# Benchmarking Redis Semantic Routing

This project aims to evaluate Redis for text classification, using the same benchmarks and metrics used by [Cunha et al](https://arxiv.org/pdf/2504.01930).

In order to process bulk classifications in order of thousands to hundreds of thousands, minor modifications are made to RedisVL classes.

These include:
1. Passing lower dimensions and higher batch-sizes to the OpenAIVectorizer 
2. Adding batch routing to the SemanticRouter
3. Batch embedding for threshold optimizer 

Open Question:
How to prevent empty preds other than setting distance threshold to an insane number like 0.9999

# Results

## Macro-F1 Scores
| Category | Dataset | **Semantic Routing** | **Traditional Approaches** |        |        | **Small Language Models (SLMs)** |        |        |        | **Large Language Models (LLMs)** |        |         |          |          |
|-----------|----------|------------------------------|----------------------------|--------|--------|----------------------------------|--------|--------|--------|----------------------------------|--------|---------|----------|----------|
|           |             | Redis | LSVM  | LR    | RF     | RoBERTa | BERT   | BART   | XLNet  | BloomZ   | LLaMa 2  | LLaMa 3.1 | Mistral  | DeepSeek |
| Topic     | DBLP        | - | 79.7(0.7) | 79.2(0.7) | 62.6(0.9) | 81.4(0.5) | 81.7(0.5) | 81.1(0.5) | 81.4(0.6) | 85.9(0.6) | 86.8(0.4) | 87.8(0.4) | 86.7(0.5) | 86.5(0.5) |
|           | Books       | - | 84.5(0.5) | 84.1(0.5) | 75.7(0.5) | 87.2(0.6) | 89.5(0.2) | 86.9(0.5) | 87.3(0.4) | 89.4(0.6) | 93.0(0.5) | 93.0(0.3) | 92.6(0.4) | 92.2(0.5) |
|           | ACM         | - | 67.7(1.5) | 66.7(1.3) | 60.1(1.2) | 70.3(1.4) | 71.8(1.0) | 70.8(0.7) | 69.9(0.9) | 72.9(1.7) | 74.9(1.9) | 77.8(0.9) | 76.3(1.4) | 75.2(1.3) |
|           | 20NG        | - | 89.8(0.6) | 89.4(0.7) | 81.6(0.5) | 86.8(0.7) | 85.4(0.5) | 87.4(0.9) | 87.4(0.8) | 87.9(0.6) | 89.9(0.6) | 90.4(0.6) | 90.3(0.7) | 89.2(0.7) |
|           | OHSUMED     | - | 72.9(1.5) | 72.0(1.4) | 56.7(1.2) | 77.8(1.2) | 76.4(1.2) | 77.6(0.7) | 77.6(1.0) | 81.5(1.0) | 82.2(0.9) | 83.1(1.1) | 83.1(0.8) | 82.0(0.9) |
|           | Reuters90   | - | 33.2(2.3) | 41.5(2.6) | 27.0(1.6) | 41.9(2.2) | 40.2(2.8) | 42.2(2.1) | 41.3(2.6) | 40.7(2.2) | 41.8(2.5) | 41.5(2.6) | 41.5(2.4) | 41.7(2.7) |
|           | WOS-11967   | - | 85.9(0.5) | 85.4(0.5) | 83.8(0.6) | 86.8(0.4) | 85.5(0.7) | 86.9(0.8) | 87.0(0.7) | 89.4(0.6) | 88.6(0.5) | 89.9(0.4) | 89.1(0.7) | 89.6(0.7) |
|           | WebKB       | - | 70.7(1.9) | 71.4(2.7) | 64.9(1.6) | 83.0(2.0) | 83.2(2.1) | 83.0(1.7) | 81.9(2.5) | 84.7(1.8) | 85.7(1.2) | 87.3(1.5) | 86.0(1.3) | 85.7(1.8) |
|           | Twitter     | - | 64.0(1.1) | 63.1(1.1) | 45.5(1.3) | 78.4(1.8) | 64.5(1.9) | 79.0(2.1) | 76.4(2.1) | 80.0(1.8) | 78.8(1.9) | 78.6(1.6) | 79.3(2.4) | 78.4(1.8) |
|           | TREC        | - | 68.8(2.5) | 68.7(2.3) | 66.0(1.8) | 95.5(0.5) | 87.6(1.4) | 95.5(0.8) | 94.3(1.1) | 96.0(0.8) | 95.9(0.6) | 96.1(0.8) | 96.0(0.8) | 96.1(0.6) |
|           | WOS-5736    | - | 91.2(0.8) | 91.7(0.8) | 91.0(0.6) | 90.5(0.9) | 89.7(1.3) | 89.6(1.7) | 90.2(0.9) | 91.3(0.8) | 89.8(0.8) | 91.9(0.5) | 91.9(0.6) | 91.4(0.9) |
| Sentiment | SST1        | - | 34.8(1.1) | 28.9(0.7) | 33.6(1.1) | 53.8(1.3) | 51.6(1.2) | 52.8(1.0) | 51.4(1.7) | 55.8(1.2) | 59.3(0.7) | 58.7(1.0) | 58.1(0.9) | 57.6(0.9) |
|           | pang_movie  | - | 77.3(0.8) | 77.0(1.0) | 75.5(0.8) | 89.0(0.4) | 87.4(0.4) | 88.1(0.5) | 88.2(0.6) | 93.3(0.3) | 93.6(0.4) | 93.6(0.4) | 93.6(0.4) | 92.9(0.2) |
|           | Movie Review| - | 75.9(0.9) | 75.7(1.2) | 73.5(0.5) | 89.0(0.7) | 87.7(0.5) | 88.2(0.6) | 86.4(3.3) | 96.5(0.4) | 93.6(0.3) | 92.0(4.0) | 93.8(0.5) | 92.8(0.4) |
|           | vader_movie | - | 78.6(0.8) | 78.5(0.6) | 76.4(0.9) | 91.3(0.5) | 88.2(0.7) | 90.4(0.6) | 90.5(0.4) | 96.2(0.3) | 95.7(0.3) | 95.8(0.4) | 95.9(0.3) | 95.1(0.5) |
|           | MPQA        | 84.8 | 78.8(0.8) | 78.5(0.8) | 78.3(0.9) | 90.2(0.8) | 89.1(0.7) | 90.1(0.7) | 88.6(0.5) | 90.6(0.6) | 91.5(0.3) | 91.5(0.5) | 91.4(0.4) | 91.1(0.4) |
|           | Subj        | - | 89.1(0.6) | 88.9(0.5) | 87.8(0.7) | 96.9(0.4) | 97.0(0.3) | 96.8(0.4) | 96.1(0.5) | 97.8(0.3) | 98.4(0.3) | 98.4(0.3) | 98.4(0.3) | 98.1(0.4) |
|           | SST2        | - | 78.9(0.6) | 79.1(0.7) | 76.9(0.4) | 93.2(0.6) | 91.5(0.6) | 92.8(0.5) | 92.1(0.4) | 95.9(0.3) | 96.5(0.4) | 96.5(0.4) | 96.4(0.5) | 96.3(0.5) |
|           | yelp_reviews| - | 94.8(0.7) | 94.9(0.8) | 87.9(0.9) | 97.9(0.4) | 95.6(0.6) | 97.5(0.4) | 97.3(0.4) | 99.1(0.2) | 99.2(0.2) | 99.4(0.1) | 99.1(0.3) | 99.1(0.2) |
| Large     | AGNews      | - | 91.8(0.3) | 91.5(0.2) | 88.1(0.3) | 94.2(0.2) | 93.9(0.2) | 93.9(0.2) | 94.0(0.1) | 95.7(0.1) | 95.4(0.1) | 95.7(0.2) | 95.0(0.1) | 95.6(0.3) |
|           | Yelp_2013   | - | 56.8(0.1) | 59.1(0.1) | 51.0(0.1) | 64.4(0.6) | 63.6(0.4) | 63.8(0.5) | 63.0(0.5) | 67.7(0.2) | 69.3(0.3) | 69.5(0.1) | 61.3(0.3) | 69.1(0.3) |
|           | MEDLINE     | - | 80.4(0.4) | 79.9(0.6) | 80.0(0.6) | 81.8(0.6) | 75.8(0.8) | 82.2(0.2) | 60.3(0.5) | 85.0(0.2) | 85.4(0.2) | 85.6(0.3) | 84.9(0.2) | 85.3(0.2) |


## Total Application Time (Training + Test)
| Category | Dataset | **Semantic Routing** | **Traditional Approaches** |        |        | **Small Language Models (SLMs)** |        |        |        | **Large Language Models (LLMs)** |        |         |          |          |
|-----------|----------|------------------------------|----------------------------|--------|--------|----------------------------------|--------|--------|--------|----------------------------------|--------|---------|----------|----------|
|           |             | Redis | LSVM  | LR    | RF     | RoBERTa | BERT   | BART   | XLNet  | BloomZ   | LLaMa 2  | LLaMa 3.1 | Mistral  | DeepSeek |
| Topic     | DBLP        | - | 118.7 | 137.5 | 1017.0 | 2354.9  | 1988.0 | 2693.7 | 3874.7 | 17960.7  | 19054.5   | 16893.5    | 17002.4   | 16993.9   |
|           | Books       | - | 86.6  | 85.2  | 1174.0 | 2075.1  | 1790.7 | 1440.7 | 3150.0 | 15847.7  | 16765.2   | 14832.2    | 14822.0   | 14883.9   |
|           | ACM         | - | 81.0  | 194.9 | 713.5  | 1430.5  | 1243.5 | 1683.7 | 2385.9 | 11712.3  | 12396.4   | 10964.2    | 10992.6   | 11013.3   |
|           | 20NG        | - | 164.2 | 159.1 | 1160.9 | 1305.9  | 1858.7 | 1597.6 | 2034.9 | 8886.7   | 9400.5    | 8349.6     | 8372.5    | 8380.4    |
|           | OHSUMED     | - | 134.4 | 126.0 | 743.9  | 1306.0  | 1142.5 | 1427.9 | 2040.9 | 8644.9   | 9160.9    | 8111.9     | 8123.5    | 8144.0    |
| Sentiment | SST1        | - | 14.4  | 10.2  | 105.4  | 386.7   | 581.4  | 438.5  | 1585.4 | 5568.6   | 5886.3    | 5217.3     | 5230.9    | 5246.9    |
|           | pang_movie  | - | 15.1  | 3.5   | 76.4   | 320.5   | 512.7  | 394.0  | 560.3  | 5011.8   | 5298.7    | 4693.5     | 4706.5    | 4717.9    |
|           | Movie Review| - | 10.1  | 1.7   | 59.7   | 316.5   | 520.9  | 399.9  | 1352.2 | 5007.9   | 5297.2    | 4694.6     | 4706.8    | 4718.0    |
|           | vader_movie | - | 12.4  | 3.4   | 73.9   | 319.0   | 510.3  | 376.0  | 555.7  | 4966.3   | 5250.7    | 4656.7     | 4665.3    | 4677.9    |
|           | MPQA        | 43.39 | 0.6   | 2.2   | 36.3   | 318.0   | 509.3  | 401.5  | 1072.5 | 4985.7   | 5268.6    | 4670.0     | 4673.1    | 4683.7    |
| Large     | AGNews      | - | 162.7 | 406.7 | 3626.3 | 8464.0  | 5468.1 | 7347.0 | 10243.6| 57728.4  | 57302.2   | 51153.5    | 50981.5   | 51184.6   |
|           | Yelp_2013   | - | 6666.6| 8319.5| 18945.7| 18023.5 | 14967.5| 22872.9| 31684.8| 152396.2 | 152396.2  | 135039.3   | 134468.5  | 130036.1  |
|           | MEDLINE     | - | 2610.2| 10015.9| 26163.8| 57354.2 | 49114.3| 90709.8| 99641.6| 368407.6 | 389803.8  | 345407.9   | 344450.1  | 345134.4  |
